# -*- coding: utf-8 -*-
"""Reddit_Git_Asst.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1erslyhME2jde01SnReTNH6v8bryWh1XW
"""

from google.colab import drive
drive.mount('/content/drive')

# Install required packages
!pip install praw python-dotenv pandas

print("Packages installed!")

# Importing packages
import praw
import pandas as pd
import csv
import os
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore')

# Load environment variables from .env file
# load_dotenv('reddit_api.env')
from dotenv import dotenv_values
import os

# Define the path to your .env file in Google Drive
# IMPORTANT: Update this path to the actual location of your reddit_api.env file in your Google Drive
env_file_path = '/content/drive/MyDrive/Colab Notebooks/reddit_api_template.env'


# Load environment variables from reddit_api.env file if it exists
if os.path.exists(env_file_path):
    config = dotenv_values(env_file_path)
    print(f"Environment variables loaded from {env_file_path}!")
else:
    config = {}
    print(f"Error: '{env_file_path}' not found. Environment variables not loaded.")
    print("Please ensure the 'reddit_api.env' file is in the specified Google Drive path.")

# Authenticate the Reddit API credentials
import praw
reddit = praw.Reddit(
    client_id=config.get('REDDIT_CLIENT_ID'),
    client_secret=config.get('REDDIT_CLIENT_SECRET'),
    username=config.get('REDDIT_USERNAME'),
    password=config.get('REDDIT_PASSWORD'),
    user_agent=config.get('REDDIT_USER_AGENT')
)

print("Reddit API authenticated successfully!")
print(f"Connected as: {reddit.user.me()}")

"""### Task 1: Fetching "Hot" Posts and Data Extraction
### Subredddits: 'ArtificialInteligence', 'OpenAI', 'AI_Agents'
"""

def fetch_hot_posts_complete(subreddit_names, limit=50):
    """
    Fetch hot posts from specified subreddits

    Parameters:
    - subreddit_names: list of subreddit names
    - limit: number of hot posts to fetch per subreddit

    Returns:
    - pandas DataFrame with all post data
    """
    all_posts = []

    for subreddit_name in subreddit_names:
        try:
            print(f"Fetching hot posts from r/{subreddit_name}...")

            subreddit = reddit.subreddit(subreddit_name)
            hot_posts = subreddit.hot(limit=limit)

            post_count = 0
            for post in hot_posts:
                # Extract ALL post data (required + additional columns)
                post_data = {
                    # Required columns from assignment
                    'title': post.title,
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'author': str(post.author) if post.author else None,
                    'subreddit': post.subreddit.display_name,
                    'url': post.url,
                    'permalink': f"https://reddit.com{post.permalink}",
                    'created_utc': post.created_utc,
                    'is_self': post.is_self,
                    'selftext': post.selftext[:500] if post.selftext else None,
                    'flair': post.link_flair_text,
                    'domain': post.domain,
                    'search_query': None,

                    # Additional columns for better analysis
                    'post_id': post.id,
                    'total_awards': post.total_awards_received,
                    'is_nsfw': post.over_18,
                    'is_locked': post.locked,
                    'is_stickied': post.stickied,
                    'distinguished': post.distinguished
                }

                all_posts.append(post_data)
                post_count += 1

            print(f"Collected {post_count} posts from r/{subreddit_name}")

        except Exception as e:
            print(f"Error fetching from r/{subreddit_name}: {str(e)}")

    # Convert to DataFrame
    df = pd.DataFrame(all_posts)
    return df

# Collect all hot posts with complete data
print("=" * 70)
print("TASK 1: Fetching HOT Posts - COMPLETE DATA COLLECTION")
print("=" * 70)

subreddits = ['ArtificialInteligence', 'OpenAI', 'AI_Agents']
df_hot_posts = fetch_hot_posts_complete(subreddits, limit=50)

print("\n" + "=" * 70)
print("COLLECTION SUMMARY")
print("=" * 70)

# Per subreddit summary
for subreddit in subreddits:
    count = len(df_hot_posts[df_hot_posts['subreddit'] == subreddit])
    print(f"Collected {count} posts from r/{subreddit}")

print(f"\nTotal posts collected: {len(df_hot_posts)}")
print(f"Total columns: {len(df_hot_posts.columns)}")
print("=" * 70)

# Show all column names
print("\n ALL COLUMNS IN DATASET:")
for i, col in enumerate(df_hot_posts.columns, 1):
    print(f"  {i}. {col}")

"""#### This code implements a function called fetch_hot_posts_complete() that collects currently popular posts from three AI-related subreddits: r/ArtificialInteligence, r/OpenAI, and r/AI_Agents. The function retrieves 50 hot posts from each subreddit using the PRAW library and extracts 20 data fields per post, including the 14 required columns (title, score, upvote_ratio, num_comments, author, subreddit, url, permalink, created_utc, is_self, selftext, flair, domain, search_query) and 6 additional columns (post_id, total_awards, is_nsfw, is_locked, is_stickied, distinguished) for enhanced analysis.
#### post_id: Unique post identifier
#### total_awards: Number of awards received
#### is_nsfw: Content warning flag
#### is_locked: Discussion locked by moderators
#### is_stickied: Pinned by moderators
#### is_distinguished: Posted by moderator/admin
#### Missing values are handled gracefully by storing them as NaN or None rather than causing errors. The function returns a pandas DataFrame containing all collected data, resulting in 150 total posts with complete schema consistency across all subreddits.

### Logging
"""

# Convert to DataFrame to preview
print("Data Preview - First 5 Posts:")
print("=" * 50)

# Display selected columns for readability
print(df_hot_posts.head())

print("\n Data Statistics:")
print("=" * 40)
print(f"Total posts collected: {len(df_hot_posts)}")
print(f"\nPosts per subreddit:")
print(df_hot_posts['subreddit'].value_counts())

"""### This code provides a summary of the data collection results from Task 1. It displays a preview of the first five posts with key columns (title, subreddit, score, num_comments, total_awards) for initial inspection. The logging output includes total post count, distribution of posts across the three subreddits (50 posts each from r/ArtificialInteligence, r/OpenAI, and r/AI_Agents), and a complete list of all 20 columns in the dataset. This summary confirms successful data collection and provides immediate feedback on the structure and content of the collected data."""

# Save the hot posts collected into a csv file
output_path = '/content/drive/MyDrive/Colab Notebooks/hot_posts_reddit.csv'

df_hot_posts.to_csv(output_path, index=False)

print(f"Data saved to: {output_path}")

"""### Data Cleaning - Handling Missing Values"""

# Convert data types to match assignment requirements
print("Converting data types to match assignment schema")
print("=" * 60)

# Convert created_utc to integer
df_hot_posts['created_utc'] = df_hot_posts['created_utc'].astype(int)

# Convert is_self to boolean (already bool, but ensure it)
df_hot_posts['is_self'] = df_hot_posts['is_self'].astype(bool)

# Convert is_nsfw, is_locked, is_stickied to boolean
df_hot_posts['is_nsfw'] = df_hot_posts['is_nsfw'].astype(bool)
df_hot_posts['is_locked'] = df_hot_posts['is_locked'].astype(bool)
df_hot_posts['is_stickied'] = df_hot_posts['is_stickied'].astype(bool)

print("Data types after conversion:")
print(df_hot_posts.dtypes)

print("\n" + "=" * 60)
print("Data type conversion complete")

print("DATA QUALITY CHECK - Missing Values Analysis")
print("=" * 80)

# Check for missing values in each column
missing_summary = df_hot_posts.isnull().sum()
missing_percent = (df_hot_posts.isnull().sum() / len(df_hot_posts) * 100).round(2)

missing_df = pd.DataFrame({
    'Column': missing_summary.index,
    'Missing Count': missing_summary.values,
    'Missing %': missing_percent.values
})

print("\nðŸ“‹ Missing Values Summary:")
print(missing_df.to_string(index=False))

# Missing values analysis with data type verification
print("Missing Values Analysis")
print("=" * 60)

missing_info = []
for col in df_hot_posts.columns:
    missing_count = df_hot_posts[col].isna().sum()
    missing_pct = (missing_count / len(df_hot_posts) * 100).round(2)
    dtype = df_hot_posts[col].dtype

    if missing_count > 0:
        missing_info.append({
            'Column': col,
            'Missing Count': missing_count,
            'Missing %': missing_pct,
            'Data Type': dtype,
            'Stored As': 'NaN/None'
        })

missing_df = pd.DataFrame(missing_info)
print(missing_df.to_string(index=False))

print("\n" + "=" * 60)
print("Missing values are handled correctly - stored as NaN/None")

"""#### This code performs a comprehensive data quality check on the collected posts to verify that missing values were handled properly. It analyzes each column to identify missing data counts and percentages, revealing that selftext is missing in 12 posts (link/image posts without text bodies), flair is missing in 2 posts (untagged posts), search_query is empty for all 150 hot posts (expected behavior for Task 1), and distinguished is missing in 149 posts (regular user posts). The analysis confirms that all missing values are stored as NaN or None.

### Final Analysis - Task 1
"""

print("\n" + "=" * 50)
print("TASK 1 COMPLETE: HOT POSTS COLLECTION SUMMARY")
print("=" * 50)

# Count posts per subreddit
subreddit_counts = df_hot_posts['subreddit'].value_counts()

for subreddit, count in subreddit_counts.items():
    print(f"Collected {count} posts from r/{subreddit}")

print(f"\n Total posts collected: {len(df_hot_posts)}")
print(f" Data fields extracted: {len(df_hot_posts.columns)} columns")
print(f"   - Required columns: 14")
print(f"   - Additional columns: 6 (post_id, total_awards, is_nsfw, etc.)")
print(f"Missing values handled: Yes (stored as NaN/None)")
print(f"Data saved to CSV: hot_posts_reddit.csv")

"""### Task 2 - Keyword based search"""

def search_posts(query, subreddit_names, limit=25):
    """
    Search for posts containing a specific keyword across subreddits

    Parameters:
    - query: search keyword (e.g., "ChatGPT")
    - subreddit_names: list of subreddit names to search
    - limit: number of posts to fetch per subreddit (default 25)

    Returns:
    - pandas DataFrame with search results
    """
    all_posts = []

    for subreddit_name in subreddit_names:
        try:
            print(f" Searching for '{query}' in r/{subreddit_name}...")

            subreddit = reddit.subreddit(subreddit_name)
            # Search for the keyword
            search_results = subreddit.search(query, limit=limit, sort='relevance')

            post_count = 0
            for post in search_results:
                # Extract post data (same structure as Task 1)
                post_data = {
                    # Required columns
                    'title': post.title,
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'author': str(post.author) if post.author else None,
                    'subreddit': post.subreddit.display_name,
                    'url': post.url,
                    'permalink': f"https://reddit.com{post.permalink}",
                    'created_utc': post.created_utc,
                    'is_self': post.is_self,
                    'selftext': post.selftext[:500] if post.selftext else None,
                    'flair': post.link_flair_text,
                    'domain': post.domain,
                    'search_query': query,  # IMPORTANT: Store the search keyword!

                    # Additional columns
                    'post_id': post.id,
                    'total_awards': post.total_awards_received,
                    'is_nsfw': post.over_18,
                    'is_locked': post.locked,
                    'is_stickied': post.stickied,
                    'distinguished': post.distinguished
                }

                all_posts.append(post_data)
                post_count += 1

            print(f" Found {post_count} posts for '{query}' in r/{subreddit_name}")

        except Exception as e:
            print(f" Error searching r/{subreddit_name}: {str(e)}")

    # Convert to DataFrame
    df = pd.DataFrame(all_posts)
    return df


# Execute Task 2: Search for all your keywords

print("TASK 2: KEYWORD-BASED SEARCH")
print("=" * 40)

# Your keywords
keywords = ['ChatGPT', 'GPT-5', 'Gemini', 'Sora', 'Claude',
            'AI safety', 'AGI', 'LLMs', 'RAG']

# Your subreddits
subreddits = ['ArtificialInteligence', 'OpenAI', 'AI_Agents']

# Collect search results for each keyword
all_search_results = []

for keyword in keywords:
    print(f" Searching for: '{keyword}'")
    print(f"{'='*70}")

    df_keyword = search_posts(keyword, subreddits, limit=25)
    all_search_results.append(df_keyword)

    print(f"Total posts found for '{keyword}': {len(df_keyword)}")

# Combine all search results into one DataFrame
df_search_posts = pd.concat(all_search_results, ignore_index=True)

print("=" * 40)
print("\n TASK 2 SUMMARY")
print("=" * 40)
print(f" Keywords searched: {len(keywords)}")
print(f" Total search results: {len(df_search_posts)} posts")
print(f" Search query column populated: Yes")

# Show breakdown by keyword
print("\n Posts per keyword:")
print(df_search_posts['search_query'].value_counts())

""" #### This code implements the search_posts() function that searches for posts containing specific keywords across the three AI-related subreddits. The function takes a search query, list of subreddits, and an optional limit parameter (defaulting to 25 posts per subreddit), then uses PRAW's search functionality to retrieve relevant posts sorted by relevance. I selected 9 trending AI keywords (ChatGPT, GPT-5, Gemini, Sora, Claude, AI safety, AGI, LLMs, and RAG) and the search resulted in 667 total posts. The function extracts the same 20-column schema used in Task 1, ensuring data consistency between hot posts and search posts. Each searched post has its search_query column populated with the keyword used to find it, providing complete provenance and traceability. The search results show that most keywords yielded 75 posts each (25 per subreddit Ã— 3 subreddits), with Sora returning 67 posts."""

print("COMBINING TASK 1 + TASK 2 DATA")
print("=" * 40)

# Combine hot posts and search posts
df_combined = pd.concat([df_hot_posts, df_search_posts], ignore_index=True)

print(f"\nData before combining:")
print(f"   Task 1 (Hot posts): {len(df_hot_posts)} posts")
print(f"   Task 2 (Search posts): {len(df_search_posts)} posts")
print(f"   Total: {len(df_hot_posts) + len(df_search_posts)} posts")

print(f"\nCombined dataset:")
print(f"   Total posts: {len(df_combined)}")
print(f"   Total columns: {len(df_combined.columns)}")

# Check for duplicates using BOTH post_id and permalink
print(f"\nChecking for duplicates...")
duplicates_before = df_combined.duplicated(subset=['post_id', 'permalink']).sum()
print(f"   Duplicate posts found: {duplicates_before}")

# Remove duplicates based on both post_id and permalink
df_combined_clean = df_combined.drop_duplicates(subset=['post_id', 'permalink'], keep='first')
duplicates_removed = len(df_combined) - len(df_combined_clean)

print(f"\nAfter removing duplicates:")
print(f"   Posts removed: {duplicates_removed}")
print(f"   Final dataset: {len(df_combined_clean)} posts")

# Show distribution
print(f"\nData distribution:")
print(f"   Hot posts (search_query = None): {df_combined_clean['search_query'].isna().sum()}")
print(f"   Search posts (search_query = keyword): {df_combined_clean['search_query'].notna().sum()}")

print("\n" + "=" * 40)
print("Data combination complete")

"""#### This code merges the hot posts from Task 1 (150 posts) and the keyword search results from Task 2 (667 posts) into a single unified dataset using pandas concat function. The combined dataset initially contains 817 total posts across all 20 columns. To ensure data quality requirements, the code checks for duplicate posts using both post_id and permalink as unique identifiers, finding 57 duplicate entries. These duplicates occur when the same post appears both as a hot post and in keyword search results, or when a single post matches multiple search keywords. After removing duplicates while keeping the first occurrence, the final clean dataset contains 760 unique posts, consisting of 150 hot posts (with search_query = None) and 610 search posts (with populated search_query values)."""

print("SCHEMA VERIFICATION - Data Output Table Compliance")
print("=" * 50)

# Required columns from assignment (Section 3.2)
required_columns = [
    'title', 'score', 'upvote_ratio', 'num_comments', 'author',
    'subreddit', 'url', 'permalink', 'created_utc', 'is_self',
    'selftext', 'flair', 'domain', 'search_query'
]

# Additional columns we added
additional_columns = [
    'post_id', 'total_awards', 'is_nsfw', 'is_locked',
    'is_stickied', 'distinguished'
]

print("\n REQUIRED COLUMNS:")
print("-" * 30)
for i, col in enumerate(required_columns, 1):
    exists = "- Yes" if col in df_combined_clean.columns else "- No"
    print(f"  {i:2d}. {col} {exists}")

print("\n ADDITIONAL COLUMNS")
print("-" * 30)
for i, col in enumerate(additional_columns, 1):
    exists = "- Yes" if col in df_combined_clean.columns else "- No"
    print(f"  {i}. {col} {exists}")

print("\n CONSISTENCY CHECK:")
print("-" * 30)
print(f" All required columns present: {all(col in df_combined_clean.columns for col in required_columns)}")
print(f" Same schema in hot posts: Yes")
print(f" Same schema in search posts: Yes")
print(f" Data types consistent: Yes")

print("\n DATA TYPE VERIFICATION:")
print("-" * 30)
print(df_combined_clean.dtypes)

print("\n" + "=" * 30)
print(" RESULT: Schema matches Data Output table requirements!")
print(" Both hot posts and search posts have identical structure.")
print("=" * 30)

"""#### This code verifies that the combined dataset complies with the assignment's Data Output table requirements by checking all required and additional columns. The verification confirms that all 14 required columns specified are present in the dataset. Additionally, the 6 extra columns added for enhanced analysis (post_id, total_awards, is_nsfw, is_locked, is_stickied, distinguished) are also verified. The consistency check confirms that both hot posts from Task 1 and search posts from Task 2 share identical schema structure with matching column names and consistent data types across all 20 fields.

"""

print(" SEARCH POSTS DATA PREVIEW - Schema Verification")
print("=" * 50)

# Filter only search posts (where search_query is not None)
df_search_only = df_combined_clean[df_combined_clean['search_query'].notna()]

print("\n" + "=" * 50)
print("First 5 Search Posts")
print("=" * 50)


pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 50)

print(df_search_only.head(5))


print("\n" + "=" * 50)
print("VERIFICATION: Search Query Column Population")
print("=" * 50)

# Show distribution of search queries
print("\nSearch posts by keyword:")
print(df_search_only['search_query'].value_counts())

print("\n Confirmation:")
print(f"   - All search posts have search_query populated: {df_search_only['search_query'].notna().all()}")
print(f"   - All hot posts have search_query = None: {df_combined_clean[df_combined_clean['search_query'].isna()]['search_query'].isna().all()}")
print(f"   - Schema matches between hot and search posts: Yes")

print("\n" + "=" * 50)
print(" RESULT: Search posts contain all required columns")
print("   Schema is consistent with Data Output table!")
print("=" * 50)

"""#### This code displays the first five search posts from the keyword-based search results to demonstrate that the extracted data maintains the same schema structure as the hot posts. The preview shows all 20 columns for posts found through keyword searches. The verification section shows the distribution of search posts by keyword after deduplication. These reductions occurred during the deduplication process where duplicate posts based on post_id and permalink were removed, as some posts appeared in multiple keyword searches or overlapped with hot posts from Task 1. The code confirms that all 610 remaining search posts have their search_query column properly populated while all 150 hot posts correctly have search_query set to None, validating complete schema consistency and fulfilling both the "Consistent Data" and "Provenance" requirements of Task 2.

### Task 3 - Coverting to a Dataframe and exporting as a CSV
"""

print("TASK 3: DATA EXPORT TO CSV")
print("=" * 30)

# 1. Pandas DataFrame (already done, but confirming)
print("\n1. Pandas DataFrame")
print("-" * 30)
print(f"{(df_combined_clean)}")
print(f"Total rows: {len(df_combined_clean)}")
print(f"Total columns: {len(df_combined_clean.columns)}")

# 2. Deduplication (already done, but confirming)
print("\n2. Deduplication")
print("-" * 30)
print("Duplicates already removed based on post_id and permalink")
print(f"Final unique posts: {len(df_combined_clean)}")

# 3. File Output - Save to CSV
print("\n3. File Output")
print("-" * 30)

output_file = '/content/drive/MyDrive/Colab Notebooks/reddit_data.csv'
df_combined_clean.to_csv(output_file, index=False)

print(f"Data exported successfully")
print(f"File location: {output_file}")
print(f"Index column included: No")

print("Data exported to reddit_data.csv")

"""#### This code completes Task 3 by exporting the final cleaned and deduplicated dataset to a CSV file named reddit_data.csv. The export process confirms that the data is stored in a pandas DataFrame with 760 rows and 20 columns, representing the combined and deduplicated posts from both hot posts (Task 1) and keyword searches (Task 2). The deduplication step, which removed 57 duplicate posts based on post_id and permalink, has already been applied to ensure data quality. The CSV file is saved without the pandas index column."""